Below is a  **clear, staged implementation plan** , tightly aligned with your *Experimentation & Evaluation* section, followed by a **precise answer on Codex usage** and a **ready-to-use Codex prompt** designed for research-grade code (not toy scripts).

---

# 1. Guiding Principle (Very Important)

Your goal is **not** to build a monolithic system first.

Your goal is to produce a **reviewer-credible, reproducible experimental pipeline** where:

* Each module maps **1-to-1** with a subsection of Section 4
* Every metric reported in the paper can be regenerated by **one command**
* Ablation configurations are explicit and scriptable

This means:

* Deterministic configs
* Clear data flow
* Minimal â€œmagicâ€

---

# 2. High-Level Implementation Roadmap (Phased)

## Phase 0 â€” Reproducibility Foundation (DO THIS FIRST)

**Objective:** Make the repo runnable before implementing intelligence.

### Tasks

1. Add:
   ```
   requirements.txt
   config/
   scripts/
   ```
2. Centralize all experiment parameters:
   * dataset paths
   * window sizes
   * model types
   * ablation flags

### Deliverables

* `config/base.yaml`
* `config/ai_only.yaml`
* `config/kg_only.yaml`
* `config/ns_full.yaml`

This mirrors **Table 6 (Ablation Study)** directly.

---

## Phase 1 â€” Data Ingestion & Preprocessing

ğŸ“„ *Maps to:* Section 4.2 + 4.3 (Datasets, Data Splits)

### Modules

```
data_processing/
 â”œâ”€â”€ preprocess_casas.py
 â”œâ”€â”€ preprocess_sphere.py
 â”œâ”€â”€ windowing.py
 â””â”€â”€ splits.py
```

### What to implement

* CASAS:
  * Parse event logs
  * Segment into 30-min rolling windows
  * Label transitions (idle â†’ meal prep)
* SPHERE:
  * Align modalities (PIR, wearable, RGB-D metadata)
  * Segment into fixed windows
  * Output tensors + labels

### Output format (IMPORTANT)

Save  **intermediate artifacts** :

```
data/processed/
 â”œâ”€â”€ casas_windows.pkl
 â”œâ”€â”€ sphere_windows.pkl
```

This ensures:

* KG-only and AI-only configs can reuse the same data
* Full reproducibility

---

## Phase 2 â€” Neural Perception Module

ğŸ“„ *Maps to:* Section 4.3 (Neural Perception) + Table 2

### Modules

```
models/
 â”œâ”€â”€ gru_casas.py
 â”œâ”€â”€ lstm_sphere.py
 â”œâ”€â”€ train.py
 â”œâ”€â”€ evaluate.py
```

### Implementation rules

* GRU â†’ CASAS (binary transition)
* LSTM â†’ SPHERE (multi-class activity)
* Sliding windows + overlap
* Deterministic seed

### Metrics to output

* Accuracy
* F1 (macro, weighted)
* Confusion matrix (saved, not plotted inline)

Save results as JSON:

```
outputs/neural_metrics.json
```

This feeds **Table 2** automatically.

---

## Phase 3 â€” Ontology & Knowledge Graph Federation

ğŸ“„ *Maps to:* Section 4.3 (Semantic Ontology and Federation)

### Modules

```
ontology/
 â””â”€â”€ smart_home.owl

kg_builder/
 â”œâ”€â”€ rdf_writer.py
 â”œâ”€â”€ casas_to_rdf.py
 â”œâ”€â”€ sphere_to_rdf.py
 â””â”€â”€ graphdb_client.py
```

### Key design decisions

* Separate named graphs:
  * `graph://casas`
  * `graph://sphere`
* Shared ontology terms
* Explicit timestamps (`hasStartTime`, `hasEndTime`)

### Metrics to collect

* Triple ingestion throughput
* Query latency
* Memory usage (RSS)

Save to:

```
outputs/semantic_metrics.json
```

This maps to **Table 3** and  **Figure 4** .

---

## Phase 4 â€” Symbolic Reasoning Engine

ğŸ“„ *Maps to:* Section 4.3 (Symbolic Reasoning)

### Modules

```
reasoning/
 â”œâ”€â”€ rules/
 â”‚    â”œâ”€â”€ anomaly.swrl
 â”‚    â”œâ”€â”€ fall_detection.swrl
 â”‚    â””â”€â”€ night_activity.swrl
 â”œâ”€â”€ symbolic_reasoner.py
 â””â”€â”€ cep_engine.py
```

### Implementation scope

* Lightweight SWRL interpreter in Python
* Temporal sliding window logic
* CEP trigger timestamps

### Metrics

* Inference correctness
* CEP latency
* Rules/minute

Save:

```
outputs/symbolic_metrics.json
```

This feeds  **Table 4** .

---

## Phase 5 â€” Feedback Loop & Adaptation

ğŸ“„ *Maps to:* Section 4.3 (Feedback Loop) + Figure 5

### Modules

```
feedback/
 â”œâ”€â”€ contradiction_detector.py
 â”œâ”€â”€ adapt_kg.py
 â”œâ”€â”€ retrain_buffer.py
 â””â”€â”€ feedback_loop.py
```

### Logic

* Detect:
  * Neural vs sensor contradiction
  * Neural vs symbolic contradiction
* Actions:
  * Retract RDF triples
  * Adjust SWRL thresholds
  * Append misclassified windows for retraining

### Metrics

* Feedback latency
* FP reduction per iteration
* Convergence cycle

Save:

```
outputs/feedback_metrics.json
```

This reproduces  **Table 5 + Figure 5** .

---

## Phase 6 â€” Experiment Orchestration (MOST IMPORTANT)

ğŸ“„ *Maps to:* Entire Section 4 + Ablation + Comparison

### Single entry point

```
evaluation/run_experiments.py
```

### Responsibilities

* Load config
* Enable/disable modules
* Run pipeline
* Aggregate metrics
* Dump final tables

Example:

```bash
python evaluation/run_experiments.py --config config/ns_full.yaml
```

This is what reviewers will run.

---

# 3. Should You Use Codex?

### Short answer: **YES â€” but only for specific layers**

### âœ… Good use cases for Codex

* Boilerplate PyTorch models
* Data loaders & windowing
* RDFLib graph construction
* SPARQL query templates
* Metric computation scripts

### âŒ Do NOT use Codex for

* Architectural decisions
* Ontology design logic
* Evaluation interpretation
* Rule semantics

You remain the  **system architect** . Codex is a  **code accelerator** , not a designer.

---

# 4. Recommended Codex Prompt (Copyâ€“Paste)

Use this  **exact style** . This is critical.

---

### ğŸ”¹ Codex Prompt: Neural + KG Research Code

```
You are a senior research software engineer assisting with a reproducible
neuro-symbolic IoT research project.

Context:
- Python 3.10
- PyTorch 2.0
- RDFLib + Owlready2
- Smart home datasets: CASAS (event-based) and SPHERE (multimodal)
- Goal: match the experimental setup and metrics of an academic paper

Task:
Implement the following module with clean, well-documented, research-grade code.

Constraints:
- Deterministic behavior (seeded)
- No hard-coded paths (use config dicts)
- Modular and testable
- Output metrics as JSON (not print-only)
- No external services required

Module to implement:
[DESCRIBE MODULE HERE, e.g. "GRU-based binary classifier for CASAS transitions"]

Inputs:
- Preprocessed sliding windows (numpy or torch tensors)
- Labels
- Config dictionary

Outputs:
- Trained model
- Evaluation metrics (accuracy, F1)
- Saved artifacts under outputs/

Do not include unnecessary abstractions.
Follow best practices for academic reproducibility.
```

---

# 5. What I Recommend We Do Next (Concrete)

**Next best step (no skipping):**

â¡ï¸ I design:

* `config/*.yaml`
* `run_experiments.py` skeleton
* metric aggregation schema

Then we implement  **Phase 1 â†’ Phase 6 sequentially** .

If you want, say:

> â€œStart with Phase 0 + Phase 1 code skeletonâ€

and I will generate  **production-ready Python files** , not pseudo-code.
