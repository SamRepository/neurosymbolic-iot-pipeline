## While you upload: what will almost certainly be “ADD” vs “MODIFY” for Phase 2

Because you already have `neurosymbolic_iot/utils/config.py` and `neurosymbolic_iot/utils/seed.py`, I will **not** replace them unless necessary. I will only ensure they expose the minimal API Phase 2 needs.

### Files you will almost certainly MODIFY

1. **`config/base.yaml`**

* Add:
  * `output.models_dir`
  * `output.eval_dir`
  * `output.fig_dir`
  * `training.casas` hyperparams (GRU)
  * `training.sphere` hyperparams (LSTM)

2. **`.gitignore`**

* Ensure these are ignored (do not push models/results):
  * `models/`
  * `evaluation/results/`
  * `evaluation/figures/`
  * already: `data/raw/`, `data/processed/`, `*.zip`, `*.rar`

3. **`requirements.txt`** (only if missing)

* Ensure you have:
  * `torch`
  * `scikit-learn`
  * `matplotlib`
  * `pyyaml`
  * `pyarrow` (for parquet)
  * plus what you already use (pandas, numpy, tqdm, etc.)

4. **`neurosymbolic_iot/utils/config.py`** (only if your current file doesn’t provide it)

* Must provide something equivalent to: `load_yaml(path) -> dict`

5. **`neurosymbolic_iot/utils/seed.py`** (only if needed)

* Must provide something equivalent to: `set_seed(seed)` that sets Python/numpy/(torch if installed)

### Files you will almost certainly ADD (Phase 2)

These do not overlap with Phase 0–1 and keep Phase 2 modular:

* `neurosymbolic_iot/models/rnn_classifier.py`

  (GRU/LSTM classifier)
* `neurosymbolic_iot/training/label_encoder.py`

  (label ↔ id mapping persisted to JSON)
* `neurosymbolic_iot/training/sequence_builders.py`

  (rebuild sequences using raw streams + window boundaries)
* `neurosymbolic_iot/training/trainer.py`

  (train loop + weighted loss for imbalance + latency measurement)
* `neurosymbolic_iot/evaluation/plots.py`

  (confusion matrix saved to PNG)
* `neurosymbolic_iot/cli/train.py`

  (`python -m neurosymbolic_iot.cli.train --dataset casas|sphere`)
* `neurosymbolic_iot/cli/evaluate.py`

  (`python -m neurosymbolic_iot.cli.evaluate --dataset casas|sphere`)

Additionally (only if missing in your repo):

* `neurosymbolic_iot/training/__init__.py`
* `neurosymbolic_iot/evaluation/__init__.py`
* `neurosymbolic_iot/models/__init__.py
  `

help me quickly the
paper LateX Overleaf rigth now for

First adapte this with
the new dataset description:

\subsection{Datasets
and Use Cases}

\label{sec:datasets}

We selected two
benchmark smart home datasets with complementary features:

\textbf{1. CASAS Kyoto
ADL Dataset} \cite{cook_casas_2025}:

Collected in a smart
apartment at Washington State University, the CASAS Kyoto ADL dataset contains
5,312 sensor events generated by 20 participants performing five scripted
activities of daily living (e.g., handwashing, meal preparation, telephone
use). The sensors include passive infrared motion detectors, item-use sensors
(e.g., microwave or tap are on/off), door contact sensors, water-flow sensors,
and stove sensors. Each event log entry is annotated with timestamps and
activity labels, making it suitable for symbolic reasoning and rule-based
activity recognition.

The adapte the
\subsubsection{Neural Perception Metrics} the part:

We report Accuracy,
macro‑ and weighted‑averaged F1 scores. Figure~\ref{fig:confusion_matrices}
displays the confusion matrices for both datasets. In the CASAS setting, most
confusion occurs between semantically adjacent activities such as “Meal Prep”
and “Housekeeping,” while transitions like “Showering” and “Sleeping” are
predicted with high precision. For SPHERE, posture-based activities (e.g.,
“Sitting” vs. “Standing”) show some overlap, though the model consistently
distinguishes dynamic states such as “Walking” and “Ambulating.” These results
support the effectiveness of the neural perception module in differentiating
fine-grained activities across heterogeneous environments.
Table~\ref{tab:perception_metrics} summarizes the results, showing that the
neural models achieve high accuracy and generalize well across activity
classes.

\begin{figure}[H]

\centering

\includegraphics[width=0.85\linewidth]{fig/Figure_3_confusion_matrices_CASAS_SPHERE_upd.pdf}

\caption{Confusion
matrices for activity recognition in SPHERE (left) and CASAS (right).}

\label{fig:confusion_matrices}

\end{figure}

\begin{table}[H]

\centering

\caption{Neural
perception performance across datasets.}

\begin{tabular}{lccc}

\toprule

\textbf{Dataset} &
\textbf{Accuracy (\%)} & \textbf{F1 (Macro)} & \textbf{F1 (Weighted)}
\\

\midrule

CASAS & 87.6 &
85.2 & 88.3 \\

SPHERE & 89.4
& 86.7 & 90.1 \\

\bottomrule

\end{tabular}

\label{tab:perception_metrics}

\end{table}
