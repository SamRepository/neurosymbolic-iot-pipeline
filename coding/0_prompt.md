Earlier you suggested the following implementation stack and repo structure:

Implementation Stack Suggestion
Layer	Technology
Data Ingestion	Python scripts
Neural Models	PyTorch or TensorFlow
KG + Ontology	RDFLib, Owlready2, GraphDB
Symbolic Reasoning	Custom rule engine or SWRL + OWL
CEP / Event Logic	custom Python or Apache Flink CEP
Real-Time Monitoring	Streamlit, Dash, or Grafana (optional)

GitHub Repository Structure
Component	File(s) or Folder	Example Path
Preprocessing Scripts	preprocess_casas.py, preprocess_sphere.py	/data_processing/
Trained Neural Models	model_gru_casas.pth, model_lstm_sphere.pth	/models/
Ontology	smart_home.owl	/ontology/
KG Generator	kg_federation_loader.py, rdf_writer.py	/kg_builder/
Rule Engine	symbolic_reasoner.py, ruleset.swrl	/reasoning/
Feedback Mechanism	feedback_loop.py, adapt_kg.py	/feedback/
Evaluation Scripts	run_experiments.py, latency_analysis.ipynb	/evaluation/

I created the structured attached in the neurosymbolic-iot-pipeline.zip file and the two datasets we gonna work on:

- CASAS Smart Home dataset and
- SPHERE house scripted dataset.

You fined below also the {Experimentation and Evaluation} section of our paper

\section{Experimentation and Evaluation}
\label{sec:experimentation_evaluation}
\subsection{Evaluation Objectives and Strategy}
\label{sec:evaluation-objectives}

The proposed neuro-symbolic IoT pipeline is evaluated based on its ability to support heterogeneous data federation, real-time inference, anomaly detection, and symbolic reasoning across smart home environments. We aim to demonstrate that integrating neural perception with symbolic reasoning over a semantic layer improves accuracy, interpretability, and adaptability under evolving conditions.

To ensure broad generalizability, we test the system using two open-source datasets from distinct smart home deployments: (i) the CASAS Kyoto ADL dataset \cite{cook_casas_2025} and (ii) the SPHERE Challenge dataset \cite{tonkin_multi-sensor_2023}. Each dataset presents different sensor modalities, annotation formats, and activity types—allowing us to validate the pipeline's semantic integration and real-time decision-making capabilities across federated sources.

The evaluation compares pipeline configurations: symbolic‑only, AI‑only, and full neuro‑symbolic. This allows us to isolate each layer’s contribution and analyze tradeoffs between performance, explainability, and latency.

\subsection{Datasets and Use Cases}
\label{sec:datasets}

We selected two benchmark smart home datasets with complementary features:

\textbf{1. CASAS Kyoto ADL Dataset} \cite{cook_casas_2025}:
Collected in a smart apartment at Washington State University, the CASAS Kyoto ADL dataset contains 5,312 sensor events generated by 20 participants performing five scripted activities of daily living (e.g., handwashing, meal preparation, telephone use). The sensors include passive infrared motion detectors, item-use sensors (e.g., microwave or tap are on/off), door contact sensors, water-flow sensors, and stove sensors. Each event log entry is annotated with timestamps and activity labels, making it suitable for symbolic reasoning and rule-based activity recognition.

\textbf{2. SPHERE Multi-modal Activity Dataset} \cite{tonkin_multi-sensor_2023}:
The SPHERE dataset combines environmental sensors (PIR motion detectors), wearable accelerometers, and RGB-D video streams. The training set consists of 10 long sequences (20–30 minutes each), while the test set comprises 872 short sequences (≈30 seconds). Activities are annotated with 20 labels (postures, ambulation, transitions, inactivity). The dataset’s multimodality enables neural perception tasks (e.g., activity recognition) and supports semantic data fusion across heterogeneous sensor types.

Both datasets are mapped to a common ontology for semantic integration and exposed as RDF triples within the knowledge graph. This allows the reasoning module to operate across sources and detect complex contextual patterns, such as detecting an anomaly when a person is inactive in a lit room while the microwave and tap are on.

\subsection{Experimental Setup}
\label{sec:setup}

The pipeline was implemented using a modular architecture consisting of five components: (i) data ingestion and preprocessing, (ii) neural perception module, (iii) semantic integration and ontology population, (iv) symbolic reasoning engine, and (v) feedback-based learning. Each module operates asynchronously on streamed data while exchanging intermediate results via standardized interfaces (e.g., JSON, RDF, or RESTful endpoints).

\textbf{Hardware and Environment.} Experiments ran on a workstation equipped with an Intel Core i7‑12700K CPU, 32 GB RAM, and an NVIDIA RTX 3060 GPU. We used Python 3.10, PyTorch 2.0 for deep learning, RDFLib for semantic modeling, and Owlready2 for ontology manipulation. The knowledge graph was stored in GraphDB 11.0 and accessed via SPARQL. The choice of RDF and GraphDB for the semantic layer was motivated by their native support for ontological reasoning, SPARQL 1.1 compliance, and efficient handling of RDF triples at scale. Unlike property graph models such as Neo4j, RDF-based stores offer superior interoperability with OWL ontologies and rule engines.

% \textbf{Neural Perception Module.} We implemented a lightweight LSTM-based model to classify activity segments in SPHERE and a GRU-based binary classifier to detect activity transitions (idle → meal preparation) in CASAS. The models operate on fixed-length windows with overlapping strides to enable real-time prediction.

\textbf{Neural Perception Module.} We implemented a lightweight LSTM-based model to classify activity segments in SPHERE and a GRU-based binary classifier to detect activity transitions (idle~$\rightarrow$~meal preparation) in CASAS. These recurrent architectures were chosen for their proven ability to capture temporal dependencies in sequential sensor data while maintaining low computational overhead—critical for real-time processing in IoT edge scenarios. Both models operate on fixed-length sliding windows with overlapping strides to enable timely and context-aware predictions under constrained latency.

% \textbf{Semantic Ontology and Federation.} A domain ontology unifies concepts from both datasets (classes like \texttt{Room}, \texttt{Sensor}, \texttt{Activity}, and \texttt{PersonState}). CASAS and SPHERE sensor streams were annotated and serialized into RDF triples, then inserted into GraphDB. Each dataset was stored as a separate named graph and federated via shared ontology terms and \texttt{owl:sameAs} links.

\textbf{Semantic Ontology and Federation.} A domain ontology unifies core concepts across both datasets, including classes such as \texttt{Room}, \texttt{Sensor}, \texttt{Activity}, and \texttt{PersonState}. Key relationships include \texttt{locatedIn(Room, Sensor)}, \texttt{performs(Person, Activity)}, and temporal predicates such as \texttt{hasStartTime}, \texttt{hasEndTime}, and \texttt{occursDuring}. These axioms support semantic alignment of heterogeneous data streams and temporal reasoning over activity traces.

To ensure extensibility, the ontology design draws from and extends established IoT ontologies, such as the Semantic Sensor Network (SSN) and SAREF models, adapting them to the specific semantics of CASAS and SPHERE environments. This approach ensures compatibility with standard vocabularies while enabling domain-specific enrichment for activity recognition and anomaly detection.

Sensor streams from both datasets were annotated and serialized into RDF triples using this unified ontology and stored in GraphDB. Each dataset was maintained in a separate named graph and federated using shared ontology classes and \texttt{owl:sameAs} links, allowing cross-source queries and reasoning. This semantic federation enables unified reasoning over distributed data sources without schema duplication or loss of contextual meaning.

% \textbf{Symbolic Reasoning.} Rules expressed in SWRL were executed using a custom Python rule engine. Temporal reasoning was implemented with time-annotated triples and sliding windows. For example, the system detects anomalies such as “motion in bedroom AND microwave ON AND no activity detected in kitchen within 5 min.”

\textbf{Symbolic Reasoning.} Rules expressed in SWRL were executed using a custom Python rule engine integrated with the semantic layer. Temporal reasoning was implemented through time-annotated RDF triples and sliding windows over event sequences.

Symbolic rules encode high-level logic grounded in the ontology, allowing the system to infer contextual states and detect abnormal patterns. For instance, consider the rule:

\textit{“IF motion detected in the bedroom AND microwave is ON AND no activity detected in the kitchen within 5 minutes, THEN flag potential anomaly.”}

Additional examples include:
\begin{itemize}
    \item \textbf{Posture-based fall detection:} \textit{“IF person transitions from standing to lying without any intermediate sitting action, AND no motion detected for 2 minutes, THEN infer possible fall event.”}
    \item \textbf{Night-time anomaly:} \textit{“IF light is ON in the kitchen between 00:00 and 04:00 AND person is not expected to be awake, THEN raise nighttime activity alert.”}
    % \item \textbf{Inactivity alerts:} \textit{“IF no motion detected in any room AND person is present AND duration exceeds 30 minutes, THEN classify as extended inactivity.”}
\end{itemize}

These symbolic inferences enhance transparency and enable domain experts to trace system decisions back to declarative logic. Moreover, symbolic outcomes can trigger downstream actions, such as logging incidents, updating activity logs, or notifying caregivers. By combining temporal context, domain axioms, and real-time observations, the symbolic module complements the neural layer with structured, explainable reasoning.

% \textbf{Feedback Loop.} When contradictions arose (e.g., predicted activity vs. sensor evidence), symbolic feedback updated the knowledge graph (retracting outdated assertions) or triggered model retraining. This closed-loop adaptation improves accuracy and reduces false positives over time.

\textbf{Feedback Loop.} The feedback mechanism operates by detecting inconsistencies between neural predictions and symbolic or sensor-derived ground truth. For example, if the neural module predicts “Sleeping” but motion is continuously detected in the kitchen, the system flags a contradiction.

Upon such events, the feedback loop executes corrective actions: (i) outdated or erroneous RDF assertions are retracted from the KG, (ii) SWRL rules may be refined (e.g., adjusted time thresholds), and (iii) the neural classifier is incrementally retrained on recent misclassifications using buffered windows.

Feedback is logged with timestamps and confidence traces to ensure traceability. This closed-loop adaptation contributes to reduced false positive rates and improves robustness under evolving conditions, as shown in Section~\ref{sec:feedback-metrics}.

% \textbf{Data Splits.} SPHERE was split 70/15/15 (training/validation/test). CASAS activities were segmented into rolling 30‑minute windows with ground truth labels. Interleaved scenarios were created to simulate concurrent activities and cross‑dataset semantic conflicts.

\textbf{Data Splits.} SPHERE was split 70/15/15 (training/validation/test). CASAS activities were segmented into rolling 30‑minute windows with ground truth labels. Interleaved scenarios were created to simulate concurrent activities and cross‑dataset semantic conflicts. This ensured realistic evaluation under varied temporal and semantic conditions.

\subsection{Evaluation Metrics}
\label{sec:metrics}

The performance of the proposed neuro‑symbolic IoT pipeline is measured using layer-specific metrics for neural perception, semantic integration, symbolic reasoning, and feedback adaptation.

\subsubsection{Neural Perception Metrics}
The neural perception module is responsible for extracting latent representations from raw sensor data and classifying human activities or anomalies. To quantify its performance, we adopt the following metrics:

\paragraph{Accuracy (\%)}
Defined as the proportion of correctly classified activity labels to the total number of predictions:
\begin{equation}
    \text{Accuracy} = \frac{1}{N} \sum_{i=1}^{N} \mathbf{1}(y_i = \hat{y}_i),
\end{equation}
where $y_i$ is the ground truth label, $\hat{y}_i$ is the predicted label, and $N$ is the total number of test instances.

\paragraph{F1 Score (Macro and Weighted)}
The F1 score captures the harmonic mean of precision and recall for each class. We report both macro-averaged and weighted-averaged F1 scores to account for label imbalance:
\begin{equation}
    \text{F1}_{\text{macro}} = \frac{1}{C} \sum_{j=1}^{C} \frac{2 \cdot P_j \cdot R_j}{P_j + R_j},
\end{equation}
where $P_j$ and $R_j$ denote precision and recall for class $j$, and $C$ is the total number of classes.

We report Accuracy,  macro‑ and weighted‑averaged F1 scores. Figure~\ref{fig:confusion_matrices} displays the confusion matrices for both datasets. In the CASAS setting, most confusion occurs between semantically adjacent activities such as “Meal Prep” and “Housekeeping,” while transitions like “Showering” and “Sleeping” are predicted with high precision. For SPHERE, posture-based activities (e.g., “Sitting” vs. “Standing”) show some overlap, though the model consistently distinguishes dynamic states such as “Walking” and “Ambulating.” These results support the effectiveness of the neural perception module in differentiating fine-grained activities across heterogeneous environments. Table~\ref{tab:perception_metrics} summarizes the results, showing that the neural models achieve high accuracy and generalize well across activity classes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{fig/Figure_3_confusion_matrices_CASAS_SPHERE_upd.pdf}
    \caption{Confusion matrices for activity recognition in SPHERE (left) and CASAS (right).}
    \label{fig:confusion_matrices}
\end{figure}

\begin{table}[H]
\centering
\caption{Neural perception performance across datasets.}
\begin{tabular}{lccc}
\toprule
\textbf{Dataset} & \textbf{Accuracy (\%)} & \textbf{F1 (Macro)} & \textbf{F1 (Weighted)} \\
\midrule
CASAS     & 87.6 & 85.2 & 88.3 \\
SPHERE    & 89.4 & 86.7 & 90.1 \\
\bottomrule
\end{tabular}
\label{tab:perception_metrics}
\end{table}

\subsubsection{Semantic Integration Metrics}
\label{sec:semantic-metrics}

To evaluate the performance and scalability of the semantic layer, we measured three key metrics: average SPARQL query latency, peak memory utilization, and RDF triple ingestion throughput.

\paragraph{Query Latency}
The average time required to retrieve a semantic assertion from the knowledge graph (KG) using SPARQL is computed as:
\begin{equation}
\text{Avg. Query Time} = \frac{1}{Q} \sum_{i=1}^{Q} (t_i^{\text{end}} - t_i^{\text{start}}),
\end{equation}
where $Q$ is the total number of issued queries.

\paragraph{Memory Utilization}
Memory consumption is assessed by tracking the peak resident set size (RSS) in megabytes during ontology load and inference execution. This metric reflects the semantic layer’s runtime footprint during federated KG access.

\paragraph{Federation Throughput}
We also monitor the throughput of semantic ingestion, computed as:
\begin{equation}
\text{Throughput} = \frac{T_{\text{inserted}}}{t_{\text{total}}}, \quad \text{(triples/sec)},
\end{equation}
where $T_{\text{inserted}}$ is the number of RDF triples generated and $t_{\text{total}}$ is the time elapsed for ingestion and federation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{fig/Figure_4_query_latency_vs_triples_small.pdf}
    \caption{Average query latency with respect to knowledge graph size.}
    \label{fig:query_latency}
\end{figure}

Figure \ref{fig:query_latency} illustrates the observed average query latency as a function of KG size. The experimental setup includes realistic RDF graphs generated from the CASAS and SPHERE datasets, with KG sizes ranging from 500 to 3,000 triples. As shown, query latency remains sublinear with KG size, staying below 50 ms even at peak load. This performance highlights the system's suitability for real-time semantic querying in resource-constrained IoT environments.

\begin{table}[H]
\centering
\caption{Semantic integration performance metrics across datasets.}
\label{tab:semantic_metrics}
\begin{tabular}{lccc}
\toprule
\textbf{Dataset} & \textbf{Query Latency (ms)} & \textbf{Memory Usage (MB)} & \textbf{Throughput (triples/sec)} \\
\midrule
CASAS     & 35.4 & 214.2 & 1,280 \\
SPHERE    & 42.7 & 289.5 & 1,095 \\
\bottomrule
\end{tabular}
\end{table}

As summarized in Table~\ref{tab:semantic_metrics}, the average SPARQL query latency remains below 50~ms even for medium-size graphs  graphs, while federation throughput exceeds 1,000 triples/sec. These results confirm that the semantic layer supports real-time reasoning and future horizontal scalable integration.

% Overall, the semantic layer demonstrates strong real-time responsiveness and horizontal scalability. Even under continuous ingestion and distributed querying, the ontology-backed triple store sustains low-latency access and efficient federation across heterogeneous IoT sources.

\subsubsection{Symbolic Reasoning Metrics}

To evaluate the performance and correctness of the symbolic reasoning engine, we define:

\paragraph{Inference Correctness}
Percentage of symbolic outputs (e.g., activity inferences, event labels) matching the ground truth:
\begin{equation}
    \text{Correctness} = \frac{I_{\text{correct}}}{I_{\text{total}}} \times 100,
\end{equation}
where $I_{\text{correct}}$ and $I_{\text{total}}$ denote the number of correct and total inferences respectively.

\paragraph{CEP Latency}
Time taken from event ingestion to rule activation, averaged across symbolic rules:
\begin{equation}
    \text{Avg. CEP Latency} = \frac{1}{R} \sum_{j=1}^{R} \Delta t_j,
\end{equation}
where $\Delta t_j$ is the elapsed time for rule $j$.

\paragraph{Rule Trigger Density}
Average number of symbolic rules activated per time window (e.g., per minute), reflecting reasoning workload:
\begin{equation}
    \text{Density} = \frac{R_{\text{triggered}}}{\Delta t}.
\end{equation}

 Table~\ref{tab:symbolic_metrics} summarizes these metrics. Correctness exceeds 88\% on both datasets, and CEP latency remains under 45 ms, demonstrating fast and accurate symbolic inference.

\begin{table}[H]
\centering
\caption{Symbolic reasoning performance metrics.}
\begin{tabular}{lccc}
\toprule
\textbf{Dataset} & \textbf{Correctness (\%)} & \textbf{CEP Latency (ms)} & \textbf{Rules/min} \\
\midrule
CASAS     & 88.5 & 42  & 5.1 \\
SPHERE    & 90.2 & 38  & 6.0 \\
\bottomrule
\end{tabular}
\label{tab:symbolic_metrics}
\end{table}

\subsubsection{Feedback Loop Metrics}
\label{sec:feedback-metrics}

This component evaluates the adaptive behavior of the system in response to misclassifications, contradictions, or environmental changes introduced during live or asynchronous reasoning.

\paragraph{Feedback Latency}
The responsiveness of adaptation is measured by the time taken from detecting an inconsistency to issuing a corrective semantic or neural update:
\begin{equation}
    \text{Feedback Latency} = t_{\text{update}} - t_{\text{error}}.
\end{equation}

\paragraph{False Positive Reduction}
We evaluate the degree to which the feedback loop improves decision reliability over time:
\begin{equation}
    \text{Reduction (\%)} = \frac{FP_{\text{initial}} - FP_{k}}{FP_{\text{initial}}} \times 100,
\end{equation}
where $FP_k$ is the number of false positives after $k$ feedback iterations.

\paragraph{Convergence Rate}
The system is said to converge when additional feedback cycles no longer yield substantial improvements in symbolic or neural accuracy. This is measured as the iteration point where the false positive curve flattens.

Figure~\ref{fig:fp_reduction} visualizes the evolution of the false positive rate over six feedback cycles. For both datasets, error reduction is steepest between the first and fourth iterations. CASAS drops from 15.0\% to 4.3\%, while SPHERE reaches a final error rate of 3.9\%. Convergence is observed by cycle 5 in CASAS and cycle 4 in SPHERE, indicating the system adapts effectively with minimal feedback overhead.
As summarized in Table~\ref{tab:feedback_metrics}, the feedback loop achieves rapid convergence with low overhead. The average feedback latency remains under 80 ms for both datasets, ensuring responsive adaptation. Final false positive rates are reduced to below 4.5\%, and convergence is typically achieved within five iterations—demonstrating the pipeline’s ability to self-correct with minimal delay or supervision.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{fig/Figure_5_false_positive_reduction_updated.pdf}
    \caption{False positive reduction over feedback cycles.}
    \label{fig:fp_reduction}
\end{figure}

\begin{table}[H]
\centering
\caption{Feedback adaptation metrics.}
\label{tab:feedback_metrics}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Value (CASAS)} & \textbf{Value (SPHERE)} \\
\midrule
Avg. feedback latency (ms) & 75 & 68 \\
Final FP rate (\%)         & 4.3 & 3.9 \\
Convergence cycle          & 5   & 4 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ablation Study}
\label{sec:ablation}

To evaluate the contribution of each module, we tested four configurations: (i) NS-Full (neural + KG + symbolic + feedback), (ii) AI-Only, (iii) KG+Rules, and (iv) NS‑NoFeedback. Table~\ref{tab:ablation_results} summarizes the results. The NS-Full configuration delivers the highest F1 score and correctness, and the lowest false positive rate, confirming that the pipeline’s layered integration and feedback are critical for performance.

\begin{table}[H]
\centering
\caption{Ablation study results across pipeline configurations.}
\label{tab:ablation_results}
\begin{tabular}{p{3.4cm} p{2cm} p{2.2cm} p{2.2cm} p{2.2cm}}
\toprule
\textbf{Configuration} & \textbf{F1 (\%)} & \textbf{Correctness (\%)} & \textbf{CEP Latency (ms)} & \textbf{FP Rate (\%)} \\
\midrule
AI-Only         & 84.3 & N/A   & N/A   & 15.2 \\
KG+Rules        & N/A  & 81.6  & 74    & 12.7 \\
NS-NoFeedback   & 87.1 & 85.9  & 58    & 10.3 \\
\textbf{NS-Full} & \textbf{89.4} & \textbf{91.8} & \textbf{48} & \textbf{4.1} \\
\bottomrule
\end{tabular}
\end{table}
\subsection{Comparative Evaluation}
\label{sec:comparison}
For context, we simulate representative neuro‑symbolic IoT architectures from prior work (Gilpin & Ilievski 2021; Amlashi et al. 2025; Rivas et al. 2022) using our datasets and approximate their performance. Table~\ref{tab:comparison_results} compares performance, latency, explainability, and adaptivity. Our NS‑Full pipeline offers superior F1 scores and latency while providing high built‑in feedback adaptation.

\begin{table}[H]
\centering
\caption{Comparative performance with state-of-the-art neuro-symbolic systems.}
\label{tab:comparison_results}
\begin{tabular}{p{3.6cm} p{2.2cm} p{2cm} p{2.5cm} p{2cm}}
\toprule
\textbf{System} & \textbf{F1 Score (\%)} & \textbf{CEP Latency (ms)} & \textbf{Explainability} & \textbf{Adaptivity} \\
\midrule
Gilpin \& Ilievski (2021) & 82.5 & 93  & Medium  & X\\
Amlashi et al. (2025)     & 85.2 & 74  & High    & ✓\\
Rivas et al. (2022)       & 86.1 & 66  & High& X\\
\textbf{Ours (NS-Full)}   & \textbf{89.4} & \textbf{48} & \textbf{High} & ✓\\
\bottomrule
\end{tabular}
\end{table}

\section{Discussion}
\label{sec:discussion}

\subsection{Interpretation of Results}

The results obtained across the CASAS and SPHERE datasets confirm the effectiveness of the proposed neuro-symbolic IoT pipeline in achieving real-time, interpretable, and federated intelligence. The neural perception module yielded high classification accuracy across both settings, demonstrating strong generalization over heterogeneous activity patterns. The semantic integration layer maintained sub-50\,ms SPARQL query latency and triple ingestion throughput exceeding 1,000 triples/sec, even as knowledge graphs grew in size. Symbolic inference preserved semantic correctness while maintaining average rule activation latency below 45\,ms, and the feedback loop reduced false positives by over 70\% within five correction cycles. These results collectively indicate that the pipeline achieves a favorable balance between responsiveness, precision, and explainability.

\subsection{Contributions and Architectural Strengths}

A major contribution of the system lies in its ability to combine neural latent representations with structured symbolic reasoning through a shared semantic backbone. Unlike isolated AI or logic-based solutions, the pipeline exploits synergies between perception and inference layers—where neural outputs enrich the knowledge graph and symbolic modules contextualize predictions. By mapping multiple sensor modalities to a shared ontology, it enables seamless reasoning across federated sources without requiring retraining. Furthermore, the use of symbolic traces, RDF-based assertions, and rule-driven alerts enhances transparency, offering machine-readable and human-readable explanations for each inference. The adaptive feedback mechanism further distinguishes the architecture, ensuring that the system can correct itself without requiring exhaustive supervision or retraining.

\subsection{Limitations and Design Trade-offs}

Nonetheless, the architecture presents several challenges that require further attention. As the symbolic rule base and ontology grow, the reasoning engine may experience increased inference overhead, potentially affecting real-time responsiveness. This trade-off between expressivity and scalability remains an open concern, particularly in multi-agent or cloud-distributed scenarios. Moreover, both CASAS and SPHERE suffer from activity annotation inconsistencies, which may introduce noise during model training and rule evaluation. Another limitation lies in the manual nature of ontology alignment; harmonizing semantic terms across datasets still requires expert intervention, limiting out-of-the-box generalization. Finally, the feedback mechanism currently operates at the segment level, leaving finer-grained corrective pathways unexplored.

\subsection{Application and Deployment Outlook}

Despite these limitations, the system demonstrates high potential for deployment in real-world IoT environments that require both automation and transparency. In healthcare or assisted-living applications, the ability to explain decisions—such as anomaly detection or user behavior changes—is essential for clinician trust and regulatory compliance. The lightweight nature of each pipeline module also allows for partial edge deployment on devices with limited resources, which is critical for latency-sensitive scenarios such as fall detection, energy control, or occupancy-based automation. Furthermore, the system’s ontology-driven design allows new sensor types to be integrated with minimal structural changes, enabling long-term extensibility.

Take into context all that to help me begin my implementation journey. Propose a plan to achieve that and tell me what ever I can use codex for this take or not. if Yes suggest a complet prompt to use with Codex
